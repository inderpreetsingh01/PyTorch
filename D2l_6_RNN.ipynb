{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNucj5Xs8kgFE2biC9E6Y7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inderpreetsingh01/PyTorch/blob/main/D2l_6_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "rcZoZCOJ6Uz9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "HNcccmgb58i7"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
        "        # Flatten a 2D list if needed\n",
        "        if tokens and isinstance(tokens[0], list):\n",
        "            tokens = [token for line in tokens for token in line]\n",
        "        # Count token frequencies\n",
        "        counter = collections.Counter(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "        # The list of unique tokens\n",
        "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
        "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
        "            return [self.idx_to_token[int(index)] for index in indices]\n",
        "        return self.idx_to_token[indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return self.token_to_idx['<unk>']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The vast ocean stretched endlessly under the bright blue sky shimmering in the sunlight Waves crashed softly along the shore creating a rhythm that felt timeless Seagulls soared high their cries echoing above while the wind carried a salty scent across the land On the horizon ships sailed gracefully their white sails billowing in the gentle breeze The beauty of nature reminded everyone there of the serenity and wonder that existed in the world if one paused to see and appreciate it'"
      ],
      "metadata": {
        "id": "cADIsNEw_Zfs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.split()\n"
      ],
      "metadata": {
        "id": "jDI-tbwN_mRp"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = [ch.lower() for word in text for ch in word] + [' ']"
      ],
      "metadata": {
        "id": "l_G8DN-Hmd6g"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "efITva4Dmn9n",
        "outputId": "83a062e9-ac40-4b00-81f2-412c3f19ebb4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['t',\n",
              " 'h',\n",
              " 'e',\n",
              " 'v',\n",
              " 'a',\n",
              " 's',\n",
              " 't',\n",
              " 'o',\n",
              " 'c',\n",
              " 'e',\n",
              " 'a',\n",
              " 'n',\n",
              " 's',\n",
              " 't',\n",
              " 'r',\n",
              " 'e',\n",
              " 't',\n",
              " 'c',\n",
              " 'h',\n",
              " 'e',\n",
              " 'd',\n",
              " 'e',\n",
              " 'n',\n",
              " 'd',\n",
              " 'l',\n",
              " 'e',\n",
              " 's',\n",
              " 's',\n",
              " 'l',\n",
              " 'y',\n",
              " 'u',\n",
              " 'n',\n",
              " 'd',\n",
              " 'e',\n",
              " 'r',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'b',\n",
              " 'r',\n",
              " 'i',\n",
              " 'g',\n",
              " 'h',\n",
              " 't',\n",
              " 'b',\n",
              " 'l',\n",
              " 'u',\n",
              " 'e',\n",
              " 's',\n",
              " 'k',\n",
              " 'y',\n",
              " 's',\n",
              " 'h',\n",
              " 'i',\n",
              " 'm',\n",
              " 'm',\n",
              " 'e',\n",
              " 'r',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " 'i',\n",
              " 'n',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 's',\n",
              " 'u',\n",
              " 'n',\n",
              " 'l',\n",
              " 'i',\n",
              " 'g',\n",
              " 'h',\n",
              " 't',\n",
              " 'w',\n",
              " 'a',\n",
              " 'v',\n",
              " 'e',\n",
              " 's',\n",
              " 'c',\n",
              " 'r',\n",
              " 'a',\n",
              " 's',\n",
              " 'h',\n",
              " 'e',\n",
              " 'd',\n",
              " 's',\n",
              " 'o',\n",
              " 'f',\n",
              " 't',\n",
              " 'l',\n",
              " 'y',\n",
              " 'a',\n",
              " 'l',\n",
              " 'o',\n",
              " 'n',\n",
              " 'g',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 's',\n",
              " 'h',\n",
              " 'o',\n",
              " 'r',\n",
              " 'e',\n",
              " 'c',\n",
              " 'r',\n",
              " 'e',\n",
              " 'a',\n",
              " 't',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " 'a',\n",
              " 'r',\n",
              " 'h',\n",
              " 'y',\n",
              " 't',\n",
              " 'h',\n",
              " 'm',\n",
              " 't',\n",
              " 'h',\n",
              " 'a',\n",
              " 't',\n",
              " 'f',\n",
              " 'e',\n",
              " 'l',\n",
              " 't',\n",
              " 't',\n",
              " 'i',\n",
              " 'm',\n",
              " 'e',\n",
              " 'l',\n",
              " 'e',\n",
              " 's',\n",
              " 's',\n",
              " 's',\n",
              " 'e',\n",
              " 'a',\n",
              " 'g',\n",
              " 'u',\n",
              " 'l',\n",
              " 'l',\n",
              " 's',\n",
              " 's',\n",
              " 'o',\n",
              " 'a',\n",
              " 'r',\n",
              " 'e',\n",
              " 'd',\n",
              " 'h',\n",
              " 'i',\n",
              " 'g',\n",
              " 'h',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'i',\n",
              " 'r',\n",
              " 'c',\n",
              " 'r',\n",
              " 'i',\n",
              " 'e',\n",
              " 's',\n",
              " 'e',\n",
              " 'c',\n",
              " 'h',\n",
              " 'o',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " 'a',\n",
              " 'b',\n",
              " 'o',\n",
              " 'v',\n",
              " 'e',\n",
              " 'w',\n",
              " 'h',\n",
              " 'i',\n",
              " 'l',\n",
              " 'e',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'w',\n",
              " 'i',\n",
              " 'n',\n",
              " 'd',\n",
              " 'c',\n",
              " 'a',\n",
              " 'r',\n",
              " 'r',\n",
              " 'i',\n",
              " 'e',\n",
              " 'd',\n",
              " 'a',\n",
              " 's',\n",
              " 'a',\n",
              " 'l',\n",
              " 't',\n",
              " 'y',\n",
              " 's',\n",
              " 'c',\n",
              " 'e',\n",
              " 'n',\n",
              " 't',\n",
              " 'a',\n",
              " 'c',\n",
              " 'r',\n",
              " 'o',\n",
              " 's',\n",
              " 's',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'l',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " 'o',\n",
              " 'n',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'h',\n",
              " 'o',\n",
              " 'r',\n",
              " 'i',\n",
              " 'z',\n",
              " 'o',\n",
              " 'n',\n",
              " 's',\n",
              " 'h',\n",
              " 'i',\n",
              " 'p',\n",
              " 's',\n",
              " 's',\n",
              " 'a',\n",
              " 'i',\n",
              " 'l',\n",
              " 'e',\n",
              " 'd',\n",
              " 'g',\n",
              " 'r',\n",
              " 'a',\n",
              " 'c',\n",
              " 'e',\n",
              " 'f',\n",
              " 'u',\n",
              " 'l',\n",
              " 'l',\n",
              " 'y',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'i',\n",
              " 'r',\n",
              " 'w',\n",
              " 'h',\n",
              " 'i',\n",
              " 't',\n",
              " 'e',\n",
              " 's',\n",
              " 'a',\n",
              " 'i',\n",
              " 'l',\n",
              " 's',\n",
              " 'b',\n",
              " 'i',\n",
              " 'l',\n",
              " 'l',\n",
              " 'o',\n",
              " 'w',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " 'i',\n",
              " 'n',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'g',\n",
              " 'e',\n",
              " 'n',\n",
              " 't',\n",
              " 'l',\n",
              " 'e',\n",
              " 'b',\n",
              " 'r',\n",
              " 'e',\n",
              " 'e',\n",
              " 'z',\n",
              " 'e',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'b',\n",
              " 'e',\n",
              " 'a',\n",
              " 'u',\n",
              " 't',\n",
              " 'y',\n",
              " 'o',\n",
              " 'f',\n",
              " 'n',\n",
              " 'a',\n",
              " 't',\n",
              " 'u',\n",
              " 'r',\n",
              " 'e',\n",
              " 'r',\n",
              " 'e',\n",
              " 'm',\n",
              " 'i',\n",
              " 'n',\n",
              " 'd',\n",
              " 'e',\n",
              " 'd',\n",
              " 'e',\n",
              " 'v',\n",
              " 'e',\n",
              " 'r',\n",
              " 'y',\n",
              " 'o',\n",
              " 'n',\n",
              " 'e',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'r',\n",
              " 'e',\n",
              " 'o',\n",
              " 'f',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 's',\n",
              " 'e',\n",
              " 'r',\n",
              " 'e',\n",
              " 'n',\n",
              " 'i',\n",
              " 't',\n",
              " 'y',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " 'w',\n",
              " 'o',\n",
              " 'n',\n",
              " 'd',\n",
              " 'e',\n",
              " 'r',\n",
              " 't',\n",
              " 'h',\n",
              " 'a',\n",
              " 't',\n",
              " 'e',\n",
              " 'x',\n",
              " 'i',\n",
              " 's',\n",
              " 't',\n",
              " 'e',\n",
              " 'd',\n",
              " 'i',\n",
              " 'n',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'w',\n",
              " 'o',\n",
              " 'r',\n",
              " 'l',\n",
              " 'd',\n",
              " 'i',\n",
              " 'f',\n",
              " 'o',\n",
              " 'n',\n",
              " 'e',\n",
              " 'p',\n",
              " 'a',\n",
              " 'u',\n",
              " 's',\n",
              " 'e',\n",
              " 'd',\n",
              " 't',\n",
              " 'o',\n",
              " 's',\n",
              " 'e',\n",
              " 'e',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " 'a',\n",
              " 'p',\n",
              " 'p',\n",
              " 'r',\n",
              " 'e',\n",
              " 'c',\n",
              " 'i',\n",
              " 'a',\n",
              " 't',\n",
              " 'e',\n",
              " 'i',\n",
              " 't']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocab(text1)"
      ],
      "metadata": {
        "id": "amDK9rWb6Qko"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.token_freqs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brz4DIHW6Szj",
        "outputId": "cd6e3116-b3ed-43cd-8bc3-a063f32a501d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('e', 64),\n",
              " ('t', 39),\n",
              " ('h', 31),\n",
              " ('s', 30),\n",
              " ('i', 30),\n",
              " ('a', 27),\n",
              " ('n', 26),\n",
              " ('r', 26),\n",
              " ('l', 21),\n",
              " ('o', 19)]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgBb5wFanZSK",
        "outputId": "af95b2e3-25c7-4370-fbab-ef83d507617c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oueYxgxEnbMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "YvEVGNkfeKSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNScratch(nn.Module):\n",
        "    \"\"\"The RNN model implemented from scratch.\"\"\"\n",
        "    def __init__(self, num_inputs, num_hiddens, vocab_size, sigma=0.01):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.W_xh = nn.Parameter(\n",
        "            torch.randn(num_inputs, num_hiddens) * sigma)\n",
        "        self.W_hh = nn.Parameter(\n",
        "            torch.randn(num_hiddens, num_hiddens) * sigma)\n",
        "        self.b_h = nn.Parameter(torch.zeros(num_hiddens))\n",
        "        self.W_hq = nn.Parameter(\n",
        "            torch.randn(num_hiddens, vocab_size) * sigma)\n",
        "        self.b_q = nn.Parameter(torch.zeros(vocab_size))\n",
        "\n",
        "\n",
        "    def forward(self, inputs, state=None):\n",
        "      inputs = self.one_hot(inputs)\n",
        "\n",
        "      if state is None:\n",
        "          # Initial state with shape: (batch_size, num_hiddens)\n",
        "          state = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
        "                            device=inputs.device)\n",
        "      else:\n",
        "          state, = state\n",
        "      outputs = []\n",
        "      # iterating over time steps (num_steps)\n",
        "      for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs)\n",
        "          state = torch.tanh(torch.matmul(X, self.W_xh) +\n",
        "                          torch.matmul(state, self.W_hh) + self.b_h)\n",
        "          outputs.append(state)\n",
        "\n",
        "      outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in outputs]\n",
        "      return torch.stack(outputs, 1), state\n",
        "\n",
        "\n",
        "    def one_hot(self, X):\n",
        "      # Output shape: (num_steps, batch_size, vocab_size)\n",
        "      return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
        "\n",
        "    def clip_gradients(self, grad_clip_val, model):\n",
        "      params = [p for p in model.parameters() if p.requires_grad]\n",
        "      norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "      if norm > grad_clip_val:\n",
        "          for param in params:\n",
        "              param.grad[:] *= grad_clip_val / norm\n",
        "\n",
        "\n",
        "    def predict(self, prefix, num_preds, vocab, device=None):\n",
        "      state, outputs = None, [vocab[prefix[0]]]\n",
        "      for i in range(len(prefix) + num_preds - 1):\n",
        "          X = torch.tensor([[outputs[-1]]], device=device)\n",
        "          Y, state = self.forward(X, state)\n",
        "          if i < len(prefix) - 1:  # Warm-up period\n",
        "              outputs.append(vocab[prefix[i + 1]])\n",
        "          else:  # Predict num_preds steps\n",
        "              # Y = self.output_layer(rnn_outputs)\n",
        "              outputs.append(int(Y.argmax(axis=2).reshape(1)))\n",
        "      return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "metadata": {
        "id": "SiP9a4oT6kzA"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_inputs, num_hiddens, num_steps = 2, 26, 32, 10\n",
        "vocab_size = 26\n",
        "rnn = RNNScratch(num_inputs, num_hiddens, vocab_size)\n",
        "# X = torch.ones((num_steps, batch_size, num_inputs))\n",
        "out = rnn.predict('who the fuck are you ', 10, vocab)\n",
        "# X = torch.ones((batch_size, num_steps), dtype=torch.int64)\n",
        "# outputs, state = rnn(X)"
      ],
      "metadata": {
        "id": "aLBiaLWeeJRE"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kZMgBmupp0cX",
        "outputId": "1878029c-36bb-4be7-8d86-a55c3fe41a8b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'who the fuck are you a<unk>zekonnnn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab['a']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvvLx7mjotjv",
        "outputId": "e9f8b1c7-f488-4d85-d87a-a986c3733961"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0ISBP9LjHE3",
        "outputId": "78da2e7b-8425-4ad3-d373-133b154f0e6d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8-x6vlwiadT",
        "outputId": "f5c59d07-77de-47af-dd48-0b4ea0ed015a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ujCNZeLtgBvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "\n",
        "class LSTMScratch(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
        "        super().__init__()\n",
        "\n",
        "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
        "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
        "                          init_weight(num_hiddens, num_hiddens),\n",
        "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
        "        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n",
        "        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n",
        "        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n",
        "        self.W_xc, self.W_hc, self.b_c = triple()  # Input node\n",
        "\n",
        "\n",
        "    def forward(self, inputs, H_C=None):\n",
        "        if H_C is None:\n",
        "            # Initial state with shape: (batch_size, num_hiddens)\n",
        "            H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
        "                          device=inputs.device)\n",
        "            C = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
        "                          device=inputs.device)\n",
        "        else:\n",
        "            H, C = H_C\n",
        "        outputs = []\n",
        "        for X in inputs:\n",
        "            I = torch.sigmoid(torch.matmul(X, self.W_xi) +\n",
        "                            torch.matmul(H, self.W_hi) + self.b_i)\n",
        "            F = torch.sigmoid(torch.matmul(X, self.W_xf) +\n",
        "                            torch.matmul(H, self.W_hf) + self.b_f)\n",
        "            O = torch.sigmoid(torch.matmul(X, self.W_xo) +\n",
        "                            torch.matmul(H, self.W_ho) + self.b_o)\n",
        "            C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +\n",
        "                              torch.matmul(H, self.W_hc) + self.b_c)\n",
        "            C = F * C + I * C_tilde\n",
        "            H = O * torch.tanh(C)\n",
        "            outputs.append(H)\n",
        "        return outputs, (H, C)"
      ],
      "metadata": {
        "id": "GXRcF_1kTiab"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8eQUiYs7T30-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU\n",
        "\n",
        "class GRUScratch(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
        "        super().__init__()\n",
        "\n",
        "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
        "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
        "                          init_weight(num_hiddens, num_hiddens),\n",
        "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
        "        self.W_xz, self.W_hz, self.b_z = triple()  # Update gate\n",
        "        self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate\n",
        "        self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state\n",
        "\n",
        "    def forward(self, inputs, H=None):\n",
        "      if H is None:\n",
        "          # Initial state with shape: (batch_size, num_hiddens)\n",
        "          H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
        "                        device=inputs.device)\n",
        "      outputs = []\n",
        "      for X in inputs:\n",
        "          Z = torch.sigmoid(torch.matmul(X, self.W_xz) +\n",
        "                          torch.matmul(H, self.W_hz) + self.b_z)\n",
        "          R = torch.sigmoid(torch.matmul(X, self.W_xr) +\n",
        "                          torch.matmul(H, self.W_hr) + self.b_r)\n",
        "          H_tilde = torch.tanh(torch.matmul(X, self.W_xh) +\n",
        "                            torch.matmul(R * H, self.W_hh) + self.b_h)\n",
        "          H = Z * H + (1 - Z) * H_tilde\n",
        "          outputs.append(H)\n",
        "      return outputs, H"
      ],
      "metadata": {
        "id": "b6tjBh7YT9Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UjbT_71qT9jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zk8lgLP3T-C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bidirectional Neural Network\n",
        "\n",
        "class BiRNNScratch(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
        "        super().__init__()\n",
        "        self.f_rnn = RNNScratch(num_inputs, num_hiddens, sigma)\n",
        "        self.b_rnn = RNNScratch(num_inputs, num_hiddens, sigma)\n",
        "        self.num_hiddens *= 2\n",
        "\n",
        "    def forward(self, inputs, Hs=None):\n",
        "      f_H, b_H = Hs if Hs is not None else (None, None)\n",
        "      f_outputs, f_H = self.f_rnn(inputs, f_H)\n",
        "      b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\n",
        "      outputs = [torch.cat((f, b), -1) for f, b in zip(\n",
        "          f_outputs, reversed(b_outputs))]\n",
        "      return outputs, (f_H, b_H)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "8NoNCaxQtfhO",
        "outputId": "0bbf55fe-24cb-4401-e8d8-203bf9768448"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ee4934508bb6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Bidirectional Neural Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBiRNNScratch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gwtikhuutf54"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}