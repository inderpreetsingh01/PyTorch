{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmuko9Q/QYPCIIo4Dy3t2S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inderpreetsingh01/PyTorch/blob/main/DL8_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "L4Sy100wQpW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_softmax(X, valid_lens):\n",
        "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
        "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
        "    def _sequence_mask(X, valid_len, value=0):\n",
        "        maxlen = X.size(1)\n",
        "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                            device=X.device)[None, :] < valid_len[:, None]\n",
        "        X[~mask] = value\n",
        "        return X\n",
        "\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "        if valid_lens.dim() == 1:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "        # On the last axis, replace masked elements with a very large negative\n",
        "        # value, whose exponentiation outputs 0\n",
        "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
      ],
      "metadata": {
        "id": "5wKz-gupQlgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZdDPsqA2QlWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SM-13bMJQlFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZUO-W1-QZgQ"
      },
      "outputs": [],
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "    \"\"\"Scaled dot product attention.\"\"\"\n",
        "    def __init__(self, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Shape of queries: (batch_size, no. of queries, d)\n",
        "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
        "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
        "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        d = queries.shape[-1]\n",
        "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries = torch.normal(0, 1, (2, 1, 2))\n",
        "keys = torch.normal(0, 1, (2, 10, 2))\n",
        "values = torch.normal(0, 1, (2, 10, 4))\n",
        "valid_lens = torch.tensor([2, 6])\n",
        "\n",
        "attention = DotProductAttention(dropout=0.5)\n",
        "attention.eval()\n",
        "d2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))"
      ],
      "metadata": {
        "id": "MQGGP3EIQzZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ndIC5KnWQ8QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    \"\"\"Additive attention.\"\"\"\n",
        "    def __init__(self, num_hiddens, dropout, **kwargs):\n",
        "        super(AdditiveAttention, self).__init__(**kwargs)\n",
        "        self.W_k = nn.LazyLinear(num_hiddens, bias=False)\n",
        "        self.W_q = nn.LazyLinear(num_hiddens, bias=False)\n",
        "        self.w_v = nn.LazyLinear(1, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
        "        # After dimension expansion, shape of queries: (batch_size, no. of\n",
        "        # queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n",
        "        # key-value pairs, num_hiddens). Sum them up with broadcasting\n",
        "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
        "        features = torch.tanh(features)\n",
        "        # There is only one output of self.w_v, so we remove the last\n",
        "        # one-dimensional entry from the shape. Shape of scores: (batch_size,\n",
        "        # no. of queries, no. of key-value pairs)\n",
        "        scores = self.w_v(features).squeeze(-1)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        # Shape of values: (batch_size, no. of key-value pairs, value\n",
        "        # dimension)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)"
      ],
      "metadata": {
        "id": "GqydVdd8Q8hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UwgKYofTQ_V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eCxi5a-tP6ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to implement the RNN decoder in the Seq2SeqAttentionDecoder class. The state of the decoder is initialized with (i) the hidden states of the last layer of the encoder at all time steps, used as keys and values for attention; (ii) the hidden state of the encoder at all layers at the final time step, which serves to initialize the hidden state of the decoder; and (iii) the valid length of the encoder, to exclude the padding tokens in attention pooling. At each decoding time step, the hidden state of the final layer of the decoder, obtained at the previous time step, is used as the query of the attention mechanism. Both the output of the attention mechanism and the input embedding are concatenated to serve as the input of the RNN decoder."
      ],
      "metadata": {
        "id": "ke-ZtSOXP2rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.attention = d2l.AdditiveAttention(num_hiddens, dropout)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(\n",
        "            embed_size + num_hiddens, num_hiddens, num_layers,\n",
        "            dropout=dropout)\n",
        "        self.dense = nn.LazyLinear(vocab_size)\n",
        "        self.apply(d2l.init_seq2seq)\n",
        "\n",
        "    def init_state(self, enc_outputs, enc_valid_lens):\n",
        "        # Shape of outputs: (num_steps, batch_size, num_hiddens).\n",
        "        # Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n",
        "        outputs, hidden_state = enc_outputs\n",
        "        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        # Shape of enc_outputs: (batch_size, num_steps, num_hiddens).\n",
        "        # Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n",
        "        enc_outputs, hidden_state, enc_valid_lens = state\n",
        "        # Shape of the output X: (num_steps, batch_size, embed_size)\n",
        "        X = self.embedding(X).permute(1, 0, 2)\n",
        "        outputs, self._attention_weights = [], []\n",
        "        for x in X:\n",
        "            # Shape of query: (batch_size, 1, num_hiddens)\n",
        "            query = torch.unsqueeze(hidden_state[-1], dim=1)\n",
        "            # Shape of context: (batch_size, 1, num_hiddens)\n",
        "            context = self.attention(\n",
        "                query, enc_outputs, enc_outputs, enc_valid_lens)\n",
        "            # Concatenate on the feature dimension\n",
        "            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n",
        "            # Reshape x as (1, batch_size, embed_size + num_hiddens)\n",
        "            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n",
        "            outputs.append(out)\n",
        "            self._attention_weights.append(self.attention.attention_weights)\n",
        "        # After fully connected layer transformation, shape of outputs:\n",
        "        # (num_steps, batch_size, vocab_size)\n",
        "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
        "        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\n",
        "                                          enc_valid_lens]\n",
        "\n",
        "    @property\n",
        "    def attention_weights(self):\n",
        "        return self._attention_weights"
      ],
      "metadata": {
        "id": "XFSZ9is9RARr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mEd1BmyeRAn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aoBrNeIhQEqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention.\"\"\"\n",
        "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = d2l.DotProductAttention(dropout)\n",
        "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        # Shape of queries, keys, or values:\n",
        "        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n",
        "        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
        "        # After transposing, shape of output queries, keys, or values:\n",
        "        # (batch_size * num_heads, no. of queries or key-value pairs,\n",
        "        # num_hiddens / num_heads)\n",
        "        queries = self.transpose_qkv(self.W_q(queries))\n",
        "        keys = self.transpose_qkv(self.W_k(keys))\n",
        "        values = self.transpose_qkv(self.W_v(values))\n",
        "\n",
        "        if valid_lens is not None:\n",
        "            # On axis 0, copy the first item (scalar or vector) for num_heads\n",
        "            # times, then copy the next item, and so on\n",
        "            valid_lens = torch.repeat_interleave(\n",
        "                valid_lens, repeats=self.num_heads, dim=0)\n",
        "\n",
        "        # Shape of output: (batch_size * num_heads, no. of queries,\n",
        "        # num_hiddens / num_heads)\n",
        "        output = self.attention(queries, keys, values, valid_lens)\n",
        "        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n",
        "        output_concat = self.transpose_output(output)\n",
        "        return self.W_o(output_concat)"
      ],
      "metadata": {
        "id": "HwiwnuNyQE-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qLSSeGC0vD8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transpose_qkv(self, X):\n",
        "    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
        "    # Shape of input X: (batch_size, no. of queries or key-value pairs,\n",
        "    # num_hiddens). Shape of output X: (batch_size, no. of queries or\n",
        "    # key-value pairs, num_heads, num_hiddens / num_heads)\n",
        "    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
        "    # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n",
        "    # pairs, num_hiddens / num_heads)\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    # Shape of output: (batch_size * num_heads, no. of queries or key-value\n",
        "    # pairs, num_hiddens / num_heads)\n",
        "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
        "\n",
        "def transpose_output(self, X):\n",
        "    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
        "    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(X.shape[0], X.shape[1], -1)"
      ],
      "metadata": {
        "id": "ZPkmOchrvEsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dEbVXfrkQFMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Position encoding"
      ],
      "metadata": {
        "id": "CA89MIGkvMDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding.\"\"\"\n",
        "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create a long enough P\n",
        "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
        "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
        "            -1, 1) / torch.pow(10000, torch.arange(\n",
        "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
        "        self.P[:, :, 0::2] = torch.sin(X)\n",
        "        self.P[:, :, 1::2] = torch.cos(X)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
        "        return self.dropout(X)"
      ],
      "metadata": {
        "id": "86qZsbjs531e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MWR9Dn-nHlY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_BPE(tokens, symbols):\n",
        "    outputs = []\n",
        "    for token in tokens:\n",
        "        start, end = 0, len(token)\n",
        "        cur_output = []\n",
        "        # Segment token with the longest possible subwords from symbols\n",
        "        while start < len(token) and start < end:\n",
        "            if token[start: end] in symbols:\n",
        "                cur_output.append(token[start: end])\n",
        "                start = end\n",
        "                end = len(token)\n",
        "            else:\n",
        "                end -= 1\n",
        "        if start < len(token):\n",
        "            cur_output.append('[UNK]')\n",
        "        outputs.append(' '.join(cur_output))\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "OkLi_4uWHmAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = ['tallest_', 'fatter_']\n",
        "symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\n",
        "print(segment_BPE(tokens, symbols))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10XENJUiHpR-",
        "outputId": "5841e76f-c1ae-4978-9b48-e789140f4274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tall e s t _', 'fa t t er_']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PmzTlreMHuHQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}